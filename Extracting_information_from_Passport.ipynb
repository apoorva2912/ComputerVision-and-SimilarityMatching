{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INTRODUCTION\n",
        "\n",
        "\n",
        "**\"Automatically extracting personal information from Passport document\".**\n",
        "\n",
        "Attributes to be extracted from each passport record:\n",
        "\n",
        "First Name (Given Name)\n",
        "\n",
        "Last Name (Surname)\n",
        "\n",
        "Date of Birth.\n",
        "\n",
        "## Organization\n",
        "\n",
        "> [Find Data Paths](#scrollTo=5gs_vOz4l5_m)\n",
        "\n",
        "> [Processing Data and Extracting Text](#scrollTo=FWVRPpNVmS1H)\n",
        "\n",
        "> [Time Elapsed for data Extraction](#scrollTo=sycN9ZJWc2g4)\n",
        "\n",
        "> [Helper functions](#scrollTo=GgpJhIQdkOiM)\n",
        "\n",
        "> [Working With Training Data](#scrollTo=gtMt60iUgKU3)\n",
        "\n",
        "> [Working with Test Data](#scrollTo=Q0UyWn7pmhNc)\n",
        "\n",
        "> [Results on Validation data](#scrollTo=LYync1q8m467)\n",
        "\n",
        "-Submitted By: Apoorva Rastogi"
      ],
      "metadata": {
        "id": "t4N-xu7Xlp4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtBo8vJsJ6-h"
      },
      "outputs": [],
      "source": [
        "#Importing Necessary Libraries\n",
        "%matplotlib inline\n",
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import difflib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from skimage.io import imread\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tesseract\n",
        "# Before Importing pytesseract Kindly insure that you have it installed as well as tesseract-ocr\n",
        "\n",
        "# The following commands worked for me\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip3 install pytesseract\n",
        "\n",
        "import pytesseract"
      ],
      "metadata": {
        "id": "RRBxcvisdZFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faad5337-7c94-49cb-df40-0deda53d7e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,181 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Data\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# I am using gdown to get data as the dataset is small and I could get data directly from the link.\n",
        "# If your data is in the drive the above lines could be used to mount the drive and access the files.\n",
        "\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/folders/1isrONXBvsIRbFOlNcXukQsVYvZKzMsIP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7r65BufKOZo",
        "outputId": "b0ee07e4-3d85-428b-af65-c148ae3aa104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Retrieving folder 1ifsllj9Hg415P4pO6jWDQc_a2ifR_yFa Test Data\n",
            "Processing file 1n2JTRUF3ZcQHAXvuWkON65irxjjfv-iB Passport_Sample15.png\n",
            "Processing file 1n26gKFCI3isteHewIkwKt49UwT5lYrQn Passport_Sample16.png\n",
            "Processing file 1n0YKK0PB17RrXJvCqRc6ztMs3E-RsARo Passport_Sample17.png\n",
            "Processing file 1mwFubtRBvHeyz3FdrPjeD2Jf4atoy5Ux Passport_Sample18.png\n",
            "Processing file 1mqYbsi5Q_coV7nxXMY69ocHyKxVPDO0V Passport_Sample19.png\n",
            "Processing file 1mo6W1umRwryOoqIleg-05yvRtvbVNKJL Passport_Sample20.png\n",
            "Processing file 1n2ocCQKz3iTrAliIDv4EsWmRuGFCg-Mm Passport_Sample21.png\n",
            "Processing file 1rjqXmLDY15r4ICuMjP8YX0CA_si1xaAO Test Data - Ground Truth.xlsx\n",
            "Retrieving folder 1ictxPMIqwXmstxQvL57-LZgoOnzsUS_b Training Data\n",
            "Processing file 1R_akIBiujm8u37XoGNHFOXdOl4qIxXOL Passport_Sample0.png\n",
            "Processing file 1JAPWZcEO8mSD8_BsxsnwE0UKHkJFGkek Passport_Sample1.png\n",
            "Processing file 1rLvH2T02UdhY6zaZs6DL787m5huSBLY2 Passport_Sample2.png\n",
            "Processing file 1jfeUe_wLfG6RsOkS-hdwZEKaeLLa8zWy Passport_Sample3.png\n",
            "Processing file 1rLGEFueEFkz2jGa7ychYJhzuS4gdRfoA Passport_Sample4.png\n",
            "Processing file 1XyZkEmDngoZHIHJ-MmjCjvEym_zqn_Rp Passport_Sample5.png\n",
            "Processing file 1J0_R2je031mRLdhskTv6oQ2NJcPNqNxM Passport_Sample6.png\n",
            "Processing file 18L2GCcn4MzwaB6ZGX1gRtM5Q2E8cnyKJ Passport_Sample7.png\n",
            "Processing file 1rjYolXrKC0MU0QMjA0RB5OLsbLH0dE5s Passport_Sample8.png\n",
            "Processing file 1hIpEreBgJHhfRL5jCJcRErl9OlyE3NK9 Passport_Sample9.png\n",
            "Processing file 13p9g6jqLyhYp0p_Mdy5pjjc0EQNb6Z-8 Passport_Sample10.png\n",
            "Processing file 1UCNWXEbqKIlEbpqmcHAGnHFQbCMiAG_4 Passport_Sample11.png\n",
            "Processing file 1osR-WSD1AluC3A9Uhl1qzv8jwPeatHU8 Passport_Sample12.png\n",
            "Processing file 1D49IdFBmQ-reMKnC9bSESCa75eBGzVBo Passport_Sample13.png\n",
            "Processing file 1k664k8_72uNfsLQVRlFwRtlevFwPnpLJ Passport_Sample14.png\n",
            "Processing file 19ay4ChuQVPiqDBCOJwpt6od3xAp2wC3Z S1_Curr.jpg\n",
            "Processing file 1Rfqd3d7HKnyZWMM-rHOVVJPozwI2MkRX S1_Old.jpg\n",
            "Processing file 1m3NyalyzMoLh6cRrHrWF_KxrW5EpFkcU S2_Curr.jpg\n",
            "Processing file 1lqZ3H8l78FcorYHRH9rtkR88zZHJA--5 S2_Old.jpg\n",
            "Processing file 1r33GSWNf929zYECvKJDRQpzK38cHXirr S3_Curr.jpg\n",
            "Processing file 1tUkkWO3SEwjjU_Bj2WC-JBpA1tmGJ5u- S3_Old.jpg\n",
            "Processing file 1ls-ViqZotRpn-usJtPmTXHyZmK1FJiv2 S4_Old.jpg\n",
            "Processing file 1bdm6kUu3t2hDJvgxYJm9DaAZ6IpV_USU Train Data - Ground Truth.xlsx\n",
            "Retrieving folder 1iiGaoTE0UtOesBOmoxuMuX5-GU7UccFX Validation Data\n",
            "Processing file 1mggmcO2WlVa7EYS-dY8t7SYnVSBVwTGC Passport_Sample22.png\n",
            "Processing file 1mgOquv0OM3s9lDa8NhpLTJ4RVC_VgHtp Passport_Sample24.jpg\n",
            "Processing file 1mdhc4bQkdnTSI7R1g9Ah0Ndpc1aSfMnh Passport_Sample26.jpg\n",
            "Processing file 1mWgnNPslqX7VQCX9wviLW79fHt-3aGB2 Passport_Sample29.jpg\n",
            "Processing file 1mN-9U4jPwkFBHUzCntxJjbrq8C6MrAg3 Passport_Sample32.jpg\n",
            "Processing file 1m5yw8HPaDg-1TAYQJxQDc8FtKWLrGBL2 Passport_Sample33.jpg\n",
            "Processing file 1mjfOAK9Nvll-Xsr26ybXe3i9H0_RJEeH Passport_Sample34.jpg\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n2JTRUF3ZcQHAXvuWkON65irxjjfv-iB\n",
            "To: /content/Passport/Test Data/Passport_Sample15.png\n",
            "100% 2.76M/2.76M [00:00<00:00, 64.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n26gKFCI3isteHewIkwKt49UwT5lYrQn\n",
            "To: /content/Passport/Test Data/Passport_Sample16.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 49.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n0YKK0PB17RrXJvCqRc6ztMs3E-RsARo\n",
            "To: /content/Passport/Test Data/Passport_Sample17.png\n",
            "100% 2.78M/2.78M [00:00<00:00, 52.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mwFubtRBvHeyz3FdrPjeD2Jf4atoy5Ux\n",
            "To: /content/Passport/Test Data/Passport_Sample18.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 52.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mqYbsi5Q_coV7nxXMY69ocHyKxVPDO0V\n",
            "To: /content/Passport/Test Data/Passport_Sample19.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 51.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mo6W1umRwryOoqIleg-05yvRtvbVNKJL\n",
            "To: /content/Passport/Test Data/Passport_Sample20.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 52.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n2ocCQKz3iTrAliIDv4EsWmRuGFCg-Mm\n",
            "To: /content/Passport/Test Data/Passport_Sample21.png\n",
            "100% 2.78M/2.78M [00:00<00:00, 52.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rjqXmLDY15r4ICuMjP8YX0CA_si1xaAO\n",
            "To: /content/Passport/Test Data/Test Data - Ground Truth.xlsx\n",
            "100% 11.7k/11.7k [00:00<00:00, 41.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1R_akIBiujm8u37XoGNHFOXdOl4qIxXOL\n",
            "To: /content/Passport/Training Data/Passport_Sample0.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 47.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JAPWZcEO8mSD8_BsxsnwE0UKHkJFGkek\n",
            "To: /content/Passport/Training Data/Passport_Sample1.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 48.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rLvH2T02UdhY6zaZs6DL787m5huSBLY2\n",
            "To: /content/Passport/Training Data/Passport_Sample2.png\n",
            "100% 2.76M/2.76M [00:00<00:00, 46.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jfeUe_wLfG6RsOkS-hdwZEKaeLLa8zWy\n",
            "To: /content/Passport/Training Data/Passport_Sample3.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 48.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rLGEFueEFkz2jGa7ychYJhzuS4gdRfoA\n",
            "To: /content/Passport/Training Data/Passport_Sample4.png\n",
            "100% 2.78M/2.78M [00:00<00:00, 51.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XyZkEmDngoZHIHJ-MmjCjvEym_zqn_Rp\n",
            "To: /content/Passport/Training Data/Passport_Sample5.png\n",
            "100% 2.75M/2.75M [00:00<00:00, 39.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J0_R2je031mRLdhskTv6oQ2NJcPNqNxM\n",
            "To: /content/Passport/Training Data/Passport_Sample6.png\n",
            "100% 2.78M/2.78M [00:00<00:00, 51.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18L2GCcn4MzwaB6ZGX1gRtM5Q2E8cnyKJ\n",
            "To: /content/Passport/Training Data/Passport_Sample7.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 52.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rjYolXrKC0MU0QMjA0RB5OLsbLH0dE5s\n",
            "To: /content/Passport/Training Data/Passport_Sample8.png\n",
            "100% 2.74M/2.74M [00:00<00:00, 50.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hIpEreBgJHhfRL5jCJcRErl9OlyE3NK9\n",
            "To: /content/Passport/Training Data/Passport_Sample9.png\n",
            "100% 2.79M/2.79M [00:00<00:00, 48.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13p9g6jqLyhYp0p_Mdy5pjjc0EQNb6Z-8\n",
            "To: /content/Passport/Training Data/Passport_Sample10.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 50.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UCNWXEbqKIlEbpqmcHAGnHFQbCMiAG_4\n",
            "To: /content/Passport/Training Data/Passport_Sample11.png\n",
            "100% 2.77M/2.77M [00:00<00:00, 44.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1osR-WSD1AluC3A9Uhl1qzv8jwPeatHU8\n",
            "To: /content/Passport/Training Data/Passport_Sample12.png\n",
            "100% 2.76M/2.76M [00:00<00:00, 47.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D49IdFBmQ-reMKnC9bSESCa75eBGzVBo\n",
            "To: /content/Passport/Training Data/Passport_Sample13.png\n",
            "100% 2.79M/2.79M [00:00<00:00, 51.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k664k8_72uNfsLQVRlFwRtlevFwPnpLJ\n",
            "To: /content/Passport/Training Data/Passport_Sample14.png\n",
            "100% 2.75M/2.75M [00:00<00:00, 52.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19ay4ChuQVPiqDBCOJwpt6od3xAp2wC3Z\n",
            "To: /content/Passport/Training Data/S1_Curr.jpg\n",
            "100% 8.75k/8.75k [00:00<00:00, 32.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rfqd3d7HKnyZWMM-rHOVVJPozwI2MkRX\n",
            "To: /content/Passport/Training Data/S1_Old.jpg\n",
            "100% 41.5k/41.5k [00:00<00:00, 82.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1m3NyalyzMoLh6cRrHrWF_KxrW5EpFkcU\n",
            "To: /content/Passport/Training Data/S2_Curr.jpg\n",
            "100% 9.87k/9.87k [00:00<00:00, 34.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lqZ3H8l78FcorYHRH9rtkR88zZHJA--5\n",
            "To: /content/Passport/Training Data/S2_Old.jpg\n",
            "100% 11.5k/11.5k [00:00<00:00, 40.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r33GSWNf929zYECvKJDRQpzK38cHXirr\n",
            "To: /content/Passport/Training Data/S3_Curr.jpg\n",
            "100% 11.8k/11.8k [00:00<00:00, 39.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tUkkWO3SEwjjU_Bj2WC-JBpA1tmGJ5u-\n",
            "To: /content/Passport/Training Data/S3_Old.jpg\n",
            "100% 13.4k/13.4k [00:00<00:00, 52.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ls-ViqZotRpn-usJtPmTXHyZmK1FJiv2\n",
            "To: /content/Passport/Training Data/S4_Old.jpg\n",
            "100% 12.1k/12.1k [00:00<00:00, 45.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bdm6kUu3t2hDJvgxYJm9DaAZ6IpV_USU\n",
            "To: /content/Passport/Training Data/Train Data - Ground Truth.xlsx\n",
            "100% 15.1k/15.1k [00:00<00:00, 22.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mggmcO2WlVa7EYS-dY8t7SYnVSBVwTGC\n",
            "To: /content/Passport/Validation Data/Passport_Sample22.png\n",
            "100% 2.78M/2.78M [00:00<00:00, 52.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mgOquv0OM3s9lDa8NhpLTJ4RVC_VgHtp\n",
            "To: /content/Passport/Validation Data/Passport_Sample24.jpg\n",
            "100% 13.3k/13.3k [00:00<00:00, 27.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mdhc4bQkdnTSI7R1g9Ah0Ndpc1aSfMnh\n",
            "To: /content/Passport/Validation Data/Passport_Sample26.jpg\n",
            "100% 9.87k/9.87k [00:00<00:00, 39.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mWgnNPslqX7VQCX9wviLW79fHt-3aGB2\n",
            "To: /content/Passport/Validation Data/Passport_Sample29.jpg\n",
            "100% 12.7k/12.7k [00:00<00:00, 37.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mN-9U4jPwkFBHUzCntxJjbrq8C6MrAg3\n",
            "To: /content/Passport/Validation Data/Passport_Sample32.jpg\n",
            "100% 13.4k/13.4k [00:00<00:00, 32.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1m5yw8HPaDg-1TAYQJxQDc8FtKWLrGBL2\n",
            "To: /content/Passport/Validation Data/Passport_Sample33.jpg\n",
            "100% 9.30k/9.30k [00:00<00:00, 34.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mjfOAK9Nvll-Xsr26ybXe3i9H0_RJEeH\n",
            "To: /content/Passport/Validation Data/Passport_Sample34.jpg\n",
            "100% 13.6k/13.6k [00:00<00:00, 48.2MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Data Paths for all Data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5gs_vOz4l5_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to get all the paths for the images to later load them\n",
        "imagepathsTrain = []\n",
        "imagepathsTest = []\n",
        "imagepathsVal = []\n",
        "# Go through all the files and subdirectories inside a folder and save path to images inside list\n",
        "for root, dirs, files in os.walk(\"/content/Passport\", topdown=False):\n",
        "  for name in files:\n",
        "    path = os.path.join(root, name)\n",
        "    if path.endswith(tuple([\"png\",\"jpg\"])): # Observing the dataset we only have the two file types.\n",
        "      if (\"Training Data\" in path) : # We want only the specified images\n",
        "           imagepathsTrain.append(path)\n",
        "      if (\"Test Data\" in path) :\n",
        "           imagepathsTest.append(path)\n",
        "      if (\"Validation Data\" in path) :\n",
        "           imagepathsVal.append(path)\n",
        "\n",
        "print(len(imagepathsTrain)) # If > 0, then an image was loaded\n",
        "print(len(imagepathsTest))\n",
        "print(len(imagepathsVal))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2cOgeNsK3Tt",
        "outputId": "08c50df4-33ec-450c-8931-7c6083d70caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "7\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Data and Extracting Text:\n",
        "\n",
        "### **Methods Used for Image Processing:**\n",
        "\n",
        "**1: GrayScaling:** Here, we use OpenCV to read the image and convert it to grayscale. Grayscale images often work better for OCR because they simplify the image and remove color noise. Depending on the specific quality issues of the image, you may need to apply additional pre-processing techniques to improve the OCR results, such as image filtering or denoising.\n",
        "\n",
        "\n",
        "\n",
        " **2: Thresholding:** Thresholding is a common image processing technique used to convert grayscale images into binary images. It helps to segment the image into regions of interest based on pixel intensity. The basic idea is to set a threshold value, and all pixels with intensity values above the threshold are set to one color (e.g., white), while all pixels with intensity values below the threshold are set to another color (e.g., black).\n",
        "\n",
        "Thresholding is useful in preprocessing images to simplify and extract important information, such as text, objects, or regions of interest. It can be used to remove noise, enhance features, or prepare the image for further analysis.[ Documentation on Thresholding](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)\n",
        "\n",
        "### **Library Used for Text Extraction:**\n",
        "\n",
        "Tesseract is an open-source OCR engine developed by Google. It can be used with Python using the pytesseract library.\n",
        "\n",
        "Before proceeding, make sure you have installed Tesseract OCR on your system. You can download it from the official GitHub repository: [tesseract-ocr]( https://github.com/tesseract-ocr/tesseract)\n",
        "\n",
        "Once Tesseract is installed, you can use the pytesseract library in Python:\n",
        "\n",
        "The installation methods mentioned above while imporitng libraries worked for me."
      ],
      "metadata": {
        "id": "FWVRPpNVmS1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getData(imagepaths): # Function Gets Data from the input Path\n",
        "\n",
        "  X =[]\n",
        "\n",
        "  for path in imagepaths:\n",
        "    img = cv2.imread(path) # Reads image and returns np.array\n",
        "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\n",
        "    ret,thresh1 = cv2.threshold(gray_image,0,255,cv2.THRESH_BINARY) # Thresholding to Improve data Extraction\n",
        "    curr=[]\n",
        "\n",
        "    # Extracting Text\n",
        "    text =pytesseract.image_to_string(thresh1)\n",
        "\n",
        "    # Extracting File Name\n",
        "    fileName = path.split('/')[4]\n",
        "    dot_position = fileName.find(\".\")\n",
        "    fileName = fileName[:dot_position]\n",
        "    curr.append(fileName)\n",
        "\n",
        "    # Processing Extracted Text\n",
        "    text = text.splitlines()\n",
        "    text = [string for string in text if string.strip() != \"\"]\n",
        "    # Skip if no text extracted\n",
        "    Text1 = text[0] if 1 < len(text) else None\n",
        "\n",
        "    if(Text1):\n",
        "      result = text[1].split(' ')[1] if 1 < len(text[1].split(' ')) else text[1]\n",
        "      curr.append(result)\n",
        "      result = text[2]\n",
        "      curr.append(result)\n",
        "      result = text[3]\n",
        "      curr.append(result)\n",
        "    else:\n",
        "      curr.append('not clear')\n",
        "      curr.append('not clear')\n",
        "      curr.append('01Jul1997')\n",
        "\n",
        "    X.append(curr)\n",
        "  return X\n",
        "\n",
        "#XTest = getData(imagepathsTest)\n",
        "#XTrain = getData(imagepathsTrain)\n",
        "#XVal  = getData(imagepathsVal)"
      ],
      "metadata": {
        "id": "8Kak66_vbE7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Time Elapsed for data Extraction\n",
        "\n",
        "import time\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Your code block to be measured\n",
        "XTrain = getData(imagepathsTrain)\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Time taken to get Training Data Values:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Your code block to be measured\n",
        "XTest = getData(imagepathsTest)\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Time taken to get Test Data Values:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Your code block to be measured\n",
        "XVal = getData(imagepathsVal)\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Time taken to get Validation Data Values:\", elapsed_time, \"seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sycN9ZJWc2g4",
        "outputId": "5b82f475-e4aa-4dac-cbd0-147edb4eba91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to get Training Data Values: 17.938997745513916 seconds\n",
            "Time taken to get Test Data Values: 7.415744304656982 seconds\n",
            "Time taken to get Validation Data Values: 3.29471755027771 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions\n",
        "\n",
        "def calculate_character_matching_percentage(string1, string2):\n",
        "    len_longer = max(len(string1), len(string2))\n",
        "\n",
        "    common_chars = sum(c1 == c2 for c1, c2 in zip(string1, string2))\n",
        "\n",
        "    matching_percentage = (common_chars / len_longer) * 100\n",
        "    return matching_percentage\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def detect_date_format(date_str):\n",
        "    formats_to_try = [\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%d\",\n",
        "        \"%m/%d/%Y\",\n",
        "        \"%d/%m/%Y\",\n",
        "        \"%m-%d-%Y\",\n",
        "        \"%d-%m-%Y\",\n",
        "    ]\n",
        "\n",
        "    for date_format in formats_to_try:\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(date_str, date_format)\n",
        "            return date_format\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    return None  # If the format couldn't be detected\n",
        "\n",
        "\n",
        "def convert_date_format(input_date):\n",
        "  try:\n",
        "    # Parse the input date string into a datetime object\n",
        "    format = detect_date_format(input_date)\n",
        "    if(format):\n",
        "      datetime_obj = datetime.strptime(input_date,format)# '%Y-%m-%d %H:%M:%S')\n",
        "    else:\n",
        "      datetime_obj = datetime.strptime(input_date,'%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Format the datetime object into the desired format\n",
        "    formatted_date = datetime_obj.strftime('%d %b %Y')\n",
        "\n",
        "    return formatted_date\n",
        "  except ValueError:\n",
        "        # Handle cases where the input does not match the expected format\n",
        "        return input_date\n",
        "\n",
        "def edit_distance(str1, str2):\n",
        "    m, n = len(str1), len(str2)\n",
        "\n",
        "    # Create a 2D array to store the edit distances\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the base case values\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Calculate the edit distance using dynamic programming\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
        "\n",
        "    return dp[m][n]\n",
        "\n"
      ],
      "metadata": {
        "id": "GgpJhIQdkOiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working With Training Data\n",
        "\n",
        "The full code Uses many similar code snippets repeated and thus run them one by one or else the results would not match. The code can be mode more modular and robust but for now it is deals with the limited data available.\n",
        "\n",
        "Accuracy of information extraction: To evaluate the model's performance in correctly extracting relevant information from the ID documents we use the below metrics:\n",
        "\n",
        "\n",
        "\n",
        "1.   Character Accuracy: The percentage of correctly recognized characters.\n",
        "2.   Word Accuracy: The percentage of correctly recognized words.\n",
        "3.   \n",
        "Edit Distance: Measures the difference between predicted text and ground truth in terms of the number of insertions, deletions, and substitutions required to match the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "gtMt60iUgKU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting Required Training Data\n",
        "TrainAns = pd.read_excel(\"/content/Passport/Training Data/Train Data - Ground Truth.xlsx\")\n",
        "Data = TrainAns[['File Name','First Name /Given Name','Last Name / Surname','Date of Birth']]\n",
        "Data['File Name'] = Data['File Name'].replace('_', '', regex=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDebt-1-QenE",
        "outputId": "5802efe2-9ba9-4cc7-b997-b0ad02c21de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3aca971e5e74>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Data['File Name'] = Data['File Name'].replace('_', '', regex=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting data For comparison\n",
        "df = pd.DataFrame(XTrain, columns = ['File Name','Last Name / Surname','First Name /Given Name','Date of Birth'])\n",
        "df['File Name'] = df['File Name'].replace('_', '', regex=True)\n",
        "#with pd.ExcelWriter('/content/Passport/extractedTrainingData.xlsx') as writer:\n",
        "#   df.to_excel(writer, sheet_name='Sheet1', index=False)"
      ],
      "metadata": {
        "id": "cIOwWQgHQlie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lastNameWordTrainMatch = 0\n",
        "lastNameCharTrainMatch = 0\n",
        "noOfChar = 0\n",
        "editDist = 0\n",
        "noOfRecords = len(df['File Name'])\n",
        "\n",
        "for i in range(len(df['File Name'])):\n",
        "  searchtext = df['File Name'][i]\n",
        "  #finding concerned row\n",
        "  concernedRow = Data[Data['File Name'].str.contains(searchtext,case=False, na=False)]\n",
        "\n",
        "  #final strings\n",
        "  string1 = str(concernedRow['Last Name / Surname'].iloc[0])\n",
        "  string2 = str(df['Last Name / Surname'][i])\n",
        "  editDist = editDist + edit_distance(string1,string2)\n",
        "  # string comparison\n",
        "  percentage = calculate_character_matching_percentage(string1, string2)\n",
        "\n",
        "  if (percentage == 100):\n",
        "    lastNameWordTrainMatch = lastNameWordTrainMatch + 1\n",
        "\n",
        "  lastNameCharTrainMatch = lastNameCharTrainMatch + (percentage * len(string1))/100\n",
        "  noOfChar = noOfChar+len(string1)\n",
        "\n",
        "\n",
        "print(f\"Percentage of Words matching in Training Data for Last Name: {lastNameWordTrainMatch/noOfRecords*100:.2f}%\")\n",
        "print(f\"Percentage of Char matching in Training Data for Last Name: {lastNameCharTrainMatch/noOfChar*100:.2f}%\")\n",
        "print(\"Edit Distance combined for all Last name in Training Data :\", editDist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jaQYGKFQ9Tb",
        "outputId": "b87a91cd-b224-4e2a-fa82-4f7d3f8da405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Words matching in Training Data for Last Name: 72.73%\n",
            "Percentage of Char matching in Training Data for Last Name: 68.94%\n",
            "Edit Distance combined for all Last name in Training Data : 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "firstNameWordTrainMatch = 0\n",
        "firstNameCharTrainMatch = 0\n",
        "noOfChar = 0\n",
        "editDist = 0\n",
        "noOfRecords = len(df['File Name'])\n",
        "for i in range(len(df['File Name'])):\n",
        "  searchtext = df['File Name'][i]\n",
        "  #finding concerned row\n",
        "  concernedRow = Data[Data['File Name'].str.contains(searchtext,case=False, na=False)]\n",
        "\n",
        "  #final strings\n",
        "  string1 = str(concernedRow['First Name /Given Name'].iloc[0])\n",
        "  string2 = str(df['First Name /Given Name'][i])\n",
        "  editDist = editDist + edit_distance(string1,string2)\n",
        "\n",
        "  percentage = calculate_character_matching_percentage(string1, string2)\n",
        "  if (percentage == 100):\n",
        "    firstNameWordTrainMatch = firstNameWordTrainMatch + 1\n",
        "\n",
        "  firstNameCharTrainMatch = firstNameCharTrainMatch + (percentage * len(string1))/100\n",
        "  noOfChar = noOfChar+len(string1)\n",
        "\n",
        "print(f\"Percentage of Words matching in Training Data for First Name: {firstNameWordTrainMatch/noOfRecords*100:.2f}%\")\n",
        "print(f\"Percentage of Char matching in Training Data for First Name: {firstNameCharTrainMatch/noOfChar*100:.2f}%\")\n",
        "print(\"Edit Distance combined for all First name in Training Data :\", editDist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8OYeSxXREd9",
        "outputId": "4f599722-ccf9-44fd-a190-66061ae42942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Words matching in Training Data for First Name: 63.64%\n",
            "Percentage of Char matching in Training Data for First Name: 67.86%\n",
            "Edit Distance combined for all First name in Training Data : 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dobWordTrainMatch = 0\n",
        "dobCharTrainMatch = 0\n",
        "noOfChar = 0\n",
        "editDist = 0\n",
        "noOfRecords = len(df['File Name'])\n",
        "\n",
        "for i in range(len(df['File Name'])):\n",
        "  searchtext = df['File Name'][i]\n",
        "  #finding concerned row\n",
        "  concernedRow = Data[Data['File Name'].str.contains(searchtext,case=False, na=False)]\n",
        "\n",
        "  #final strings\n",
        "  string1 = str(concernedRow['Date of Birth'].iloc[0])\n",
        "  output_date_str = convert_date_format(string1)\n",
        "  string2 = str(df['Date of Birth'][i])\n",
        "  editDist = editDist + edit_distance(output_date_str,string2)\n",
        "\n",
        "  percentage = calculate_character_matching_percentage(output_date_str, string2)\n",
        "\n",
        "  if (percentage == 100):\n",
        "    dobWordTrainMatch = dobWordTrainMatch + 1\n",
        "\n",
        "  dobCharTrainMatch = dobCharTrainMatch + (percentage * len(string1))/100\n",
        "  noOfChar = noOfChar+len(string1)\n",
        "\n",
        "\n",
        "print(f\"Percentage of Words matching in Training Data for DOB: {dobWordTrainMatch/noOfRecords*100:.2f}%\")\n",
        "print(f\"Percentage of Char matching in Training Data for DOB: {dobCharTrainMatch/noOfChar*100:.2f}%\")\n",
        "print(\"Edit Distance combined for all DOB in Training Data :\", editDist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOrlKHJgRRfJ",
        "outputId": "94e7477d-da01-4700-d9b7-30bb19053418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Words matching in Training Data for DOB: 45.45%\n",
            "Percentage of Char matching in Training Data for DOB: 63.64%\n",
            "Edit Distance combined for all DOB in Training Data : 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Test Data\n",
        "\n",
        "\n",
        "As we are using a pretrained model we find the same metric for testing data as well\n",
        "Accuracy of information extraction: To evaluate the model's performance in correctly extracting relevant information from the ID documents we use the below metrics:\n",
        "\n",
        "\n",
        "\n",
        "1.   Character Accuracy: The percentage of correctly recognized characters.\n",
        "2.   Word Accuracy: The percentage of correctly recognized words.\n",
        "3.   \n",
        "Edit Distance: Measures the difference between predicted text and ground truth in terms of the number of insertions, deletions, and substitutions required to match the text.\n"
      ],
      "metadata": {
        "id": "Q0UyWn7pmhNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting Required Test Data\n",
        "TrainAns = pd.read_excel(\"/content/Passport/Test Data/Test Data - Ground Truth.xlsx\")\n",
        "Data = TrainAns[['File Name','First Name /Given Name','Last Name / Surname','Date of Birth']]\n",
        "Data['File Name'] = Data['File Name'].replace('_', '', regex=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjgxJ52gmlj7",
        "outputId": "a8bd65b8-d157-4464-f2a3-9476dab64b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-aadddeb58384>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Data['File Name'] = Data['File Name'].replace('_', '', regex=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting data For comparison\n",
        "df = pd.DataFrame(XTest, columns = ['File Name','Last Name / Surname','First Name /Given Name','Date of Birth'])\n",
        "df['File Name'] = df['File Name'].replace('_', '', regex=True)\n",
        "\n",
        "with pd.ExcelWriter('/content/Passport/extractedTestData.xlsx') as writer:\n",
        "    df.to_excel(writer, sheet_name='Sheet1', index=False)\n"
      ],
      "metadata": {
        "id": "tFUlm9kcm4gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lastNameWordTestMatch = 0\n",
        "lastNameCharTestMatch = 0\n",
        "noOfChar = 0\n",
        "editDist = 0\n",
        "noOfRecords = len(df['File Name'])\n",
        "\n",
        "for i in range(len(df['File Name'])):\n",
        "  searchtext = df['File Name'][i]\n",
        "  #finding concerned row\n",
        "  concernedRow = Data[Data['File Name'].str.contains(searchtext,case=False, na=False)]\n",
        "\n",
        "  string1 = str(concernedRow['Last Name / Surname'].iloc[0])\n",
        "  string2 = str(df['Last Name / Surname'][i])\n",
        "  editDist = editDist + edit_distance(string1,string2)\n",
        "\n",
        "  percentage = calculate_character_matching_percentage(string1, string2)\n",
        "\n",
        "  if (percentage == 100):\n",
        "    lastNameWordTestMatch = lastNameWordTestMatch + 1\n",
        "\n",
        "  lastNameCharTestMatch = lastNameCharTestMatch + (percentage * len(string1))/100\n",
        "  noOfChar = noOfChar+len(string1)\n",
        "\n",
        "#print(f\"Percentage of character matching: {percentage:.2f}%\")\n",
        "\n",
        "\n",
        "print(f\"Percentage of Words matching in Test Data for Last Name: {lastNameWordTestMatch/noOfRecords*100:.2f}%\")\n",
        "print(f\"Percentage of Char matching in Test Data for Last Name: {lastNameCharTestMatch/noOfChar*100:.2f}%\")\n",
        "print(\"Edit Distance combined for all Last name in Test Data :\", editDist)\n",
        "# This should be 100 % but the models is not matching for PassportSample16 and on further observation\n",
        "# The Ground Truth is Incorrect.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWlkW-9rm4pz",
        "outputId": "49c4caa6-e025-40bd-d1c9-75cbbc380033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Words matching in Test Data for Last Name: 85.71%\n",
            "Percentage of Char matching in Test Data for Last Name: 85.71%\n",
            "Edit Distance combined for all Last name in Test Data : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "firstNameWordTestMatch = 0\n",
        "firstNameCharTestMatch = 0\n",
        "noOfChar = 0\n",
        "editDist = 0\n",
        "noOfRecords = len(df['File Name'])\n",
        "for i in range(len(df['File Name'])):\n",
        "  searchtext = df['File Name'][i]\n",
        "  concernedRow = Data[Data['File Name'].str.contains(searchtext,case=False, na=False)]\n",
        "\n",
        "\n",
        "  string1 = str(concernedRow['First Name /Given Name'].iloc[0])\n",
        "  string2 = str(df['First Name /Given Name'][i])\n",
        "  editDist = editDist + edit_distance(string1,string2)\n",
        "\n",
        "  percentage = calculate_character_matching_percentage(string1, string2)\n",
        "\n",
        "  if (percentage == 100):\n",
        "    firstNameWordTestMatch = firstNameWordTestMatch + 1\n",
        "\n",
        "  firstNameCharTestMatch = firstNameCharTestMatch + (percentage * len(string1))/100\n",
        "  noOfChar = noOfChar+len(string1)\n",
        "\n",
        "#print(f\"Percentage of character matching: {percentage:.2f}%\")\n",
        "\n",
        "\n",
        "print(f\"Percentage of Words matching in Test Data for First Name: {firstNameWordTestMatch/noOfRecords*100:.2f}%\")\n",
        "print(f\"Percentage of Char matching in Test Data for First Name: {firstNameCharTestMatch/noOfChar*100:.2f}%\")\n",
        "print(\"Edit Distance combined for all First name in Test Data :\", editDist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNy4Kfwsm4tq",
        "outputId": "cf87c184-b6de-4daa-e6e6-61629b8a6b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Words matching in Test Data for First Name: 85.71%\n",
            "Percentage of Char matching in Test Data for First Name: 82.05%\n",
            "Edit Distance combined for all First name in Test Data : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dobWordTestMatch = 0\n",
        "dobCharTestMatch = 0\n",
        "noOfChar = 0\n",
        "editDist = 0\n",
        "noOfRecords = len(df['File Name'])\n",
        "\n",
        "for i in range(len(df['File Name'])):\n",
        "  searchtext = df['File Name'][i]\n",
        "  concernedRow = Data[Data['File Name'].str.contains(searchtext,case=False, na=False)]\n",
        "\n",
        "\n",
        "  string1 = str(concernedRow['Date of Birth'].iloc[0])\n",
        "  output_date_str = convert_date_format(string1)\n",
        "  string2 = str(df['Date of Birth'][i])\n",
        "  editDist = editDist + edit_distance(output_date_str,string2)\n",
        "\n",
        "  percentage = calculate_character_matching_percentage(output_date_str, string2)\n",
        "\n",
        "  if (percentage == 100):\n",
        "    dobWordTestMatch = dobWordTestMatch + 1\n",
        "\n",
        "  dobCharTestMatch = dobCharTestMatch + (percentage * len(string1))/100\n",
        "  noOfChar = noOfChar+len(string1)\n",
        "\n",
        "\n",
        "print(f\"Percentage of Words matching in Test Data for DOB: {dobWordTestMatch/noOfRecords*100:.2f}%\")\n",
        "print(f\"Percentage of Char matching in Test Data for DOB: {dobCharTestMatch/noOfChar*100:.2f}%\")\n",
        "print(\"Edit Distance combined for all DOB in Test Data :\", editDist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4hAI2gHm4x9",
        "outputId": "2130815e-e698-40eb-ba5d-3664c2ef5c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Words matching in Test Data for DOB: 71.43%\n",
            "Percentage of Char matching in Test Data for DOB: 84.42%\n",
            "Edit Distance combined for all DOB in Test Data : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results on Validation data\n",
        "\n",
        "#Since we do not have final solution for Validation Set We could not Match it\n",
        "# Observing below we can see that only one data is extracted from the images present\n",
        "df = pd.DataFrame(XVal, columns = ['File Name','Last Name / Surname','First Name /Given Name','Date of Birth'])\n",
        "df['File Name'] = df['File Name'].replace('_', '', regex=True)\n",
        "\n",
        "with pd.ExcelWriter('/content/Passport/extractedValData.xlsx') as writer:\n",
        "    df.to_excel(writer, sheet_name='Sheet1', index=False)"
      ],
      "metadata": {
        "id": "LYync1q8m467"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Work\n",
        "\n",
        "\n",
        "Explore ways to get information out of old and dark images."
      ],
      "metadata": {
        "id": "dKY8QBwxYIcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOT PART OF SUBMISSION\n",
        "\n",
        "Trails works to get information from old and dark Images. Need to study topic indepth for better solution"
      ],
      "metadata": {
        "id": "dYWtdBeKqxbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "# Additional Trails to extract data from not clear Images\n",
        "# Need more time to work this out and try other methods for this task\n",
        "def enhance_image(input_image_path):\n",
        "    # Read the image\n",
        "    #image = cv2.imread(input_image_path)\n",
        "    image = input_image_path\n",
        "\n",
        "    # Check if the image is loaded successfully\n",
        "    if image is None:\n",
        "        print(\"Error: Unable to load the image.\")\n",
        "        return\n",
        "\n",
        "    # Apply image filtering (e.g., using a bilateral filter)\n",
        "    filtered_image = cv2.bilateralFilter(image, d=1, sigmaColor=50, sigmaSpace=50)\n",
        "\n",
        "    return filtered_image\n",
        "\n",
        "# Example usage:\n",
        "input_image_path = \"/content/Passport/Validation Data/Passport_Sample34.jpg\"\n",
        "#enhance_image(input_image_path)\n",
        "\n",
        "\n",
        "#img = cv2.imread(\"/content/Passport/Validation Data/Passport_Sample34.jpg\")\n",
        "img = cv2.imread('/content/Passport/Validation Data/Passport_Sample34.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "#gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "ret,thresh1 = cv2.threshold(gray_image,160,255,cv2.THRESH_BINARY)\n",
        "#cv2_imshow(thresh1 )\n",
        "myimg = enhance_image(thresh1)\n",
        "#cv2_imshow(myimg)\n",
        "#print(pytesseract.image_to_string(myimg))\n"
      ],
      "metadata": {
        "id": "aiQDEKkDRuXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install pytesseract"
      ],
      "metadata": {
        "id": "JyewwP5jKOdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XVal = [] # Image data\n",
        "\n",
        "\n",
        "for path in imagepathsVal:\n",
        "    img = cv2.imread(path) # Reads image and returns np.array\n",
        "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\n",
        "    ret,thresh1 = cv2.threshold(gray_image,150,255,cv2.THRESH_BINARY) # Thresholding to Improve data Extraction\n",
        "    thresholded_img = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                           cv2.THRESH_BINARY, 11, 2)\n",
        "    curr=[]\n",
        "\n",
        "    # Extracting Text\n",
        "    text =pytesseract.image_to_string(thresholded_img)\n",
        "    #print(text)\n",
        "\n",
        "    # Extracting File Name\n",
        "    fileName = path.split('/')[4]\n",
        "    dot_position = fileName.find(\".\")\n",
        "    fileName = fileName[:dot_position]\n",
        "    curr.append(fileName)\n",
        "\n",
        "    #print(text)\n",
        "    #cv2_imshow(thresholded_img)\n",
        "\n",
        "    # Processing Extracted Text\n",
        "    text = text.splitlines()\n",
        "    text = [string for string in text if string.strip() != \"\"]\n",
        "    # Skip if no text extracted\n",
        "    Text1 = text[0] if 1 < len(text) else None\n",
        "\n",
        "''' if(Text1):\n",
        "      result = text[1].split(' ')[1] if 1 < len(text[1].split(' ')) else text[1]\n",
        "      curr.append(result)\n",
        "      result = text[2]\n",
        "      curr.append(result)\n",
        "      result = text[3]\n",
        "      curr.append(result)\n",
        "    else:\n",
        "      curr.append('not clear')\n",
        "      curr.append('not clear')\n",
        "      curr.append('01Jul1997')\n",
        "\n",
        "    XVal.append(curr)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "psrKmJ32KOgx",
        "outputId": "d5772f87-0612-4203-83a4-c91f67510073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" if(Text1):\\n      result = text[1].split(' ')[1] if 1 < len(text[1].split(' ')) else text[1]\\n      curr.append(result)\\n      result = text[2]\\n      curr.append(result)\\n      result = text[3]\\n      curr.append(result)\\n    else:\\n      curr.append('not clear')\\n      curr.append('not clear')\\n      curr.append('01Jul1997')\\n\\n    XVal.append(curr)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install keras-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uev6vevmtsvo",
        "outputId": "2a03c29f-8681-4aa0-aecf-d97fb494b38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-ocr\n",
            "  Downloading keras_ocr-0.8.9-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/41.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (0.6.2)\n",
            "Collecting efficientnet==1.0.0 (from keras-ocr)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Collecting essential_generators (from keras-ocr)\n",
            "  Downloading essential_generators-1.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fonttools in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (4.41.1)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (0.4.0)\n",
            "Collecting pyclipper (from keras-ocr)\n",
            "  Downloading pyclipper-1.3.0.post4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m813.9/813.9 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (4.65.0)\n",
            "Collecting validators (from keras-ocr)\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet==1.0.0->keras-ocr)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet==1.0.0->keras-ocr) (0.19.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (1.10.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (4.7.0.72)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (2.25.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators->keras-ocr) (4.4.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (3.8.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (2023.7.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (2.8.2)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19580 sha256=d9415cb2d8bb3a1f13447716a64fe998b7e66a2dde77f4d854ad18d3955a3774\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
            "Successfully built validators\n",
            "Installing collected packages: pyclipper, essential_generators, validators, keras-applications, efficientnet, keras-ocr\n",
            "Successfully installed efficientnet-1.0.0 essential_generators-1.0 keras-applications-1.0.8 keras-ocr-0.8.9 pyclipper-1.3.0.post4 validators-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras_ocr\n",
        "\n",
        "# keras-ocr will automatically download pretrained\n",
        "# weights for the detector and recognizer.\n",
        "pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "# Get a set of three example images\n",
        "#images = [\n",
        " #   keras_ocr.tools.read(url) for url in [\n",
        " #       'https://upload.wikimedia.org/wikipedia/commons/b/bd/Army_Reserves_Recruitment_Banner_MOD_45156284.jpg',\n",
        " #       'https://upload.wikimedia.org/wikipedia/commons/e/e8/FseeG2QeLXo.jpg',\n",
        " #       'https://upload.wikimedia.org/wikipedia/commons/b/b4/EUBanana-500x112.jpg'\n",
        " #   ]\n",
        "#]\n",
        "for path in imagepathsVal:\n",
        "    img = cv2.imread(path) # Reads image and returns np.array\n",
        "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\n",
        "    ret,thresh1 = cv2.threshold(gray_image,150,255,cv2.THRESH_BINARY) # Thresholding to Improve data Extraction\n",
        "    curr=[]\n",
        "# Each list of predictions in prediction_groups is a list of\n",
        "# (word, box) tuples.\n",
        "#prediction_groups = pipeline.recognize(thresh1)\n",
        "\n",
        "# Plot the predictions\n",
        "#fig, axs = plt.subplots(nrows=len(img), figsize=(20, 20))\n",
        "#for ax, image, predictions in zip(axs, img, prediction_groups):\n",
        "#    keras_ocr.tools.drawAnnotations(image=image, predictions=predictions, ax=ax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32W7xAsMt2Dz",
        "outputId": "cf476438-557b-40b3-ab7b-37385aefa352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n"
          ]
        }
      ]
    }
  ]
}